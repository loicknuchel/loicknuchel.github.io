---
date:     2016-10-01
name:     Criteo
duration: oct. 2016 - dec. 2018, 2 years and 3 months
website:  https://www.criteo.com
logo:     /assets/img/logos/criteo.png
twitter:  CriteoEng
tags:     [Scala developer, Scrum master, React, Data catalog]
---

I was looking for a new Scala opportunity in a challenging environment.
After a lot of hesitations, I choose **Criteo** to work with [Guillaume Bort](https://www.linkedin.com/in/guillaumebort){:target="_blank"},
the creator of [Play framework](https://www.playframework.com){:target="_blank"}.

My team worked on a timeseries scheduler ([Cuttle](https://github.com/criteo/cuttle){:target="_blank"}) to manage all the data jobs at **Criteo**,
waiting for data availability to have a correct output with the lowest possible latency.
I mainly worked on **Dataflow**, a **Cuttle** overlay targeting analysts with predefined jobs allowing to remove all the complexities putting scheduled jobs in production.

Discussing with [Arnaud de Turckheim](https://www.linkedin.com/in/adeturckheim), focusing on analysts productivity,
we noticed that they lost a lot of time looking for data to use and understanding them.
Many created some documentation (in spreadsheets, confluence...) but not maintained on the long run.

So thanks, the 10% projects we launched **DataDoc**, a data catalog focused on business documentation, automatically enhanced with related data.
It was eagerly awaited and, from what I heard, is quite successful right now with a team having taken ownership on it.
I received some compliments from the new maintainers for the code understandability and documentation
even if I used pure FP, Event sourcing, CQRS and some other concepts considered as complex and could not do any knowledge sharing.
So, this is possible to do good code that new developers like, it's not a fatality ^^

At some time, the Lake team (maintaining the Criteo Hadoop cluster) asked for mentoring to launch a Scala project from scratch (as they are DevOps).
I proposed myself and were chosen so I could build Mumak with them, a project to hadoop resiliency at cluster level. The goal was to copy data from a cluster to an other,
in a timely, efficient and meaningful way; at scale (peta byte).<br>
Great work, great success <i class="emoji thumbs-up"></i>

On the side, I regularly gave internal trainings about Scala (as well as [external ones](https://www.humancoders.com/formations/scala){:target="_blank"}!, not related to Criteo ^^).
And also participated to many internal events such as hackathons (building a desk map), did talks at all hands or tech lunch and animated coding dojo sessions.
